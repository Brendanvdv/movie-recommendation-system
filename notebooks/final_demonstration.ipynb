{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendation System Using Hybrid Similarity\n",
    "\n",
    "**Course:** 02807 Computational Tools for Data Science  \n",
    "**Institution:** Technical University of Denmark (DTU)  \n",
    "**Semester:** Autumn 2024\n",
    "\n",
    "---\n",
    "\n",
    "## Project Summary\n",
    "\n",
    "This project implements a hybrid movie recommendation system that combines multiple similarity dimensions. Using the Rotten Tomatoes dataset, we demonstrate:\n",
    "\n",
    "1. **Course Topic 1 - Similar Items:** We compute pairwise similarity across movies using six distinct feature types, combined through weighted averaging.\n",
    "\n",
    "2. **Course Topic 2 - Clustering:** K-Means clustering groups movies by genre characteristics, with evaluation using Davies-Bouldin index and silhouette scores.\n",
    "\n",
    "3. **Outside Topic - Topic Modeling (LDA):** We use Latent Dirichlet Allocation to discover latent topics from critic reviews, enabling automatic genre label generation based on review content.\n",
    "\n",
    "The system produces recommendations by computing a hybrid similarity score:\n",
    "\n",
    "$$S_{hybrid} = \\alpha \\cdot S_{info} + \\beta \\cdot S_{rating} + \\gamma \\cdot S_{genre} + \\delta \\cdot S_{year} + \\epsilon \\cdot S_{style} + \\zeta \\cdot S_{type}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Dependencies](#1-setup-and-dependencies)\n",
    "2. [Data Loading and Preprocessing](#2-data-loading-and-preprocessing)\n",
    "3. [Feature Engineering and Vectorization](#3-feature-engineering-and-vectorization)\n",
    "4. [Topic Modeling - LDA (Outside Topic)](#4-topic-modeling---lda-outside-topic)\n",
    "5. [Similarity Matrix Construction](#5-similarity-matrix-construction)\n",
    "6. [Hybrid Scoring System](#6-hybrid-scoring-system)\n",
    "7. [Clustering Analysis (Course Topic 2)](#7-clustering-analysis-course-topic-2)\n",
    "8. [Final Recommendation System](#8-final-recommendation-system)\n",
    "9. [Results and Evaluation](#9-results-and-evaluation)\n",
    "10. [Conclusion](#10-conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup and Dependencies\n",
    "\n",
    "This section imports all required libraries for the project. Key dependencies include:\n",
    "- **sentence-transformers** for creating text embeddings\n",
    "- **scikit-learn** for similarity computation, clustering, and evaluation metrics\n",
    "- **gensim** for LDA topic modeling\n",
    "- **nltk** for text preprocessing (stopwords, lemmatization)\n",
    "\n",
    "The global configuration defines parameters used throughout the project, including genre categories, content rating types, minimum review threshold, and number of LDA topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if running for the first time)\n",
    "# !pip install sentence-transformers pandas numpy scikit-learn matplotlib seaborn gensim nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Brend\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "# Sentence Transformers (for embeddings)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Topic Modeling (Outside Topic)\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "%matplotlib inline\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking 17 genres and 5 content ratings\n",
      "LDA will discover 15 latent topics\n"
     ]
    }
   ],
   "source": [
    "# All possible genres in the dataset\n",
    "ALL_GENRES = [\n",
    "    'science fiction & fantasy', 'drama', 'western', 'comedy', 'classics',\n",
    "    'action & adventure', 'kids & family', 'musical & performing arts',\n",
    "    'documentary', 'art house & international', 'horror', 'sports & fitness',\n",
    "    'faith & spirituality', 'mystery & suspense', 'animation', 'special interest', 'romance'\n",
    "]\n",
    "\n",
    "ALL_AGE_RATINGS = ['pg', 'r', 'g', 'pg-13', 'nc17']\n",
    "MIN_REVIEWS = 5\n",
    "NUM_TOPICS = 15\n",
    "\n",
    "print(f\"Tracking {len(ALL_GENRES)} genres and {len(ALL_AGE_RATINGS)} content ratings\")\n",
    "print(f\"LDA will discover {NUM_TOPICS} latent topics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "This section loads the Rotten Tomatoes dataset and groups critic reviews by movie. Each movie must have at least 5 reviews to be included in the analysis. The grouped data structure contains movie metadata (title, year, genres, content rating) along with all associated critic reviews and review types (fresh/rotten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset() -> pd.DataFrame:\n",
    "    \"\"\"Load the final dataset\"\"\"\n",
    "    dataset = '../datasets/final_dataset.csv'\n",
    "    final_dataset = pd.read_csv(dataset)\n",
    "    return final_dataset\n",
    "\n",
    "def group_movies(final_dataset, min_reviews=MIN_REVIEWS):\n",
    "    \"\"\"Group reviews by movie\"\"\"\n",
    "    grouped = final_dataset.groupby('rotten_tomatoes_link')\n",
    "    movie_data = []\n",
    "\n",
    "    for movie_id, group in grouped:\n",
    "        reviews = group['review_content'].tolist()\n",
    "        critic_names = group['critic_name'].to_list()\n",
    "        review_types = group['review_type'].to_list()\n",
    "        first_row = group.iloc[0]\n",
    "\n",
    "        movie_data.append({\n",
    "            'movie_id': movie_id,\n",
    "            'movie_title': first_row['movie_title'],\n",
    "            'content_rating': first_row['content_rating'],\n",
    "            'genres': first_row['genres'],\n",
    "            'year': int(first_row['original_release_date']) if str(first_row['original_release_date']).isdigit() else 0,\n",
    "            'movie_info': first_row['movie_info'],\n",
    "            'reviews': reviews,\n",
    "            'critic_names': critic_names,\n",
    "            'review_types': review_types,\n",
    "            'combined_review_text': ' '.join(reviews)\n",
    "        })\n",
    "    \n",
    "    movie_data_filtered = [m for m in movie_data if len(m['reviews']) >= min_reviews]\n",
    "    print(f\"Loaded {len(movie_data_filtered)} movies with >= {min_reviews} reviews\")\n",
    "    return movie_data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15504 movies with >= 5 reviews\n"
     ]
    }
   ],
   "source": [
    "# Load and process dataset\n",
    "final_dataset = get_dataset()\n",
    "movie_data = group_movies(final_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Feature Engineering and Vectorization\n",
    "\n",
    "This section transforms movie data into numerical representations that can be compared mathematically. We use the sentence-transformers library to create 384-dimensional embeddings for movie descriptions and critic reviews. Additional features include binary encodings for genres and content ratings, normalized release years, and fresh/rotten review patterns. The vectorization process takes 10-20 minutes for the full dataset, so results are cached for faster subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(movie_data: list, model) -> list[dict]:\n",
    "    \"\"\"Create embeddings for content and review style\"\"\"\n",
    "    \n",
    "    def encode_genres(movie_genres: list, all_genres: list):\n",
    "        return [1 if genre in movie_genres else 0 for genre in all_genres]\n",
    "    \n",
    "    def encode_content_rating(content_rating, all_content_ratings):\n",
    "        return [1 if cr == content_rating else 0 for cr in all_content_ratings]\n",
    "    \n",
    "    def encode_review_type(review_types):\n",
    "        return [1 if rt == \"fresh\" else 0 for rt in review_types]\n",
    "    \n",
    "    years = [movie['year'] for movie in movie_data if movie['year'] > 0]\n",
    "    min_year = min(years)\n",
    "    max_year = max(years)\n",
    "\n",
    "    total_movies = len(movie_data)\n",
    "\n",
    "    for i, movie in enumerate(movie_data, start=1):\n",
    "        if i % 500 == 0 or i == total_movies:\n",
    "            print(f\"Processing movie {i}/{total_movies} ({i/total_movies*100:.1f}%)\")\n",
    "\n",
    "        movie['movie_info_embeddings'] = model.encode(movie['movie_info'])\n",
    "        movie['content_rating_norm'] = encode_content_rating(movie['content_rating'], ALL_AGE_RATINGS)\n",
    "        movie['genre_vector'] = encode_genres(movie['genres'], ALL_GENRES)\n",
    "        movie['year_norm'] = (movie['year'] - min_year) / (max_year - min_year)\n",
    "        \n",
    "        review_embeddings = model.encode(movie['reviews'])\n",
    "        movie['avg_review_embeddings'] = review_embeddings.mean(axis=0)\n",
    "        movie['review_types_norm'] = encode_review_type(movie['review_types'])\n",
    "    \n",
    "    return movie_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from ../cache/vectorized_movie_data.pkl...\n",
      "Loaded 15504 movies\n"
     ]
    }
   ],
   "source": [
    "# Load or create vectorized data\n",
    "VECTORIZED_PATH = '../cache/vectorized_movie_data.pkl'\n",
    "\n",
    "if os.path.exists(VECTORIZED_PATH):\n",
    "    print(f\"Loading from {VECTORIZED_PATH}...\")\n",
    "    with open(VECTORIZED_PATH, 'rb') as f:\n",
    "        vectorized_data = pickle.load(f)\n",
    "    \n",
    "    # Ensure combined_review_text exists (for backward compatibility)\n",
    "    for movie in vectorized_data:\n",
    "        if 'combined_review_text' not in movie:\n",
    "            movie['combined_review_text'] = ' '.join(movie['reviews'])\n",
    "else:\n",
    "    print(\"Vectorizing data...\")\n",
    "    st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    vectorized_data = vectorize(movie_data, st_model)\n",
    "    with open(VECTORIZED_PATH, 'wb') as f:\n",
    "        pickle.dump(vectorized_data, f)\n",
    "        \n",
    "print(f\"Loaded {len(vectorized_data)} movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Topic Modeling - LDA (Outside Topic)\n",
    "\n",
    "### 4.1 Introduction to LDA\n",
    "\n",
    "**Latent Dirichlet Allocation (LDA)** is a generative probabilistic model that discovers latent topics in a collection of documents. Each document is represented as a mixture of topics, and each topic is a distribution over words.\n",
    "\n",
    "**Why LDA for movie recommendations:**\n",
    "- Discovers hidden thematic patterns in critic reviews\n",
    "- Generates interpretable genre-like labels automatically\n",
    "- Captures nuanced movie characteristics beyond predefined genre categories\n",
    "\n",
    "**Implementation details:**\n",
    "- Text preprocessing removes stopwords and applies lemmatization\n",
    "- We train an LDA model with 15 topics on the combined movie descriptions and critic reviews\n",
    "- Each movie receives a topic distribution vector, indicating the probability of belonging to each discovered topic\n",
    "- Topic labels are manually mapped to interpretable genre-like categories based on the most prominent words in each topic\n",
    "- The top 3 topics (with >5% probability) become the movie's generated genre labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup text preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update({\"movie\", \"film\", \"films\", \"story\", \"character\", \"characters\"})\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z ]\", \" \", str(text))\n",
    "    return text.lower()\n",
    "\n",
    "def preprocess_for_lda(text):\n",
    "    tokens = simple_preprocess(text, deacc=True)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained LDA model...\n"
     ]
    }
   ],
   "source": [
    "# Build or load LDA model\n",
    "LDA_MODEL_PATH = '../cache/lda_model.pkl'\n",
    "LDA_DICT_PATH = '../cache/lda_dictionary.pkl'\n",
    "\n",
    "if os.path.exists(LDA_MODEL_PATH) and os.path.exists(LDA_DICT_PATH):\n",
    "    print(\"Loading pre-trained LDA model...\")\n",
    "    with open(LDA_MODEL_PATH, 'rb') as f:\n",
    "        lda_model = pickle.load(f)\n",
    "    with open(LDA_DICT_PATH, 'rb') as f:\n",
    "        dictionary = pickle.load(f)\n",
    "else:\n",
    "    print(\"Training LDA model...\")\n",
    "    texts_for_lda = []\n",
    "    for movie in vectorized_data:\n",
    "        combined = movie['movie_title'] + ' ' + movie['movie_info'] + ' ' + movie['combined_review_text']\n",
    "        tokens = preprocess_for_lda(clean_text(combined))\n",
    "        texts_for_lda.append(tokens)\n",
    "        movie['tokens'] = tokens\n",
    "    \n",
    "    dictionary = Dictionary(texts_for_lda)\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.5)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in texts_for_lda]\n",
    "    \n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=NUM_TOPICS,\n",
    "                         random_state=42, passes=10, alpha='auto', eta='auto')\n",
    "    \n",
    "    with open(LDA_MODEL_PATH, 'wb') as f:\n",
    "        pickle.dump(lda_model, f)\n",
    "    with open(LDA_DICT_PATH, 'wb') as f:\n",
    "        pickle.dump(dictionary, f)\n",
    "    print(\"LDA model trained and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered Topics:\n",
      "================================================================================\n",
      "Topic  0: war, drama, history, true, historical, world, powerful, men\n",
      "Topic  1: action, thriller, plot, fun, bad, game, bond, better\n",
      "Topic  2: music, musical, rock, song, love, age, young, teen\n",
      "Topic  3: thriller, drama, crime, dark, violence, western, genre, man\n",
      "Topic  4: sequel, franchise, full, review, fun, series, first, star\n",
      "Topic  5: drama, love, woman, emotional, full, family, two, review\n",
      "Topic  6: mystery, thriller, french, art, play, style, plot, hitchcock\n",
      "Topic  7: sport, robert, scorsese, drama, dance, howard, old, game\n",
      "Topic  8: documentary, subject, political, fascinating, man, people, american, world\n",
      "Topic  9: comedy, funny, romantic, laugh, cast, love, script, two\n",
      "Topic 10: sci, science, fiction, space, stone, alien, idea, scott\n",
      "Topic 11: funny, comedy, laugh, joke, bad, humor, satire, minute\n",
      "Topic 12: kid, family, child, animated, animation, disney, adult, tale\n",
      "Topic 13: action, effect, world, epic, art, visual, movie, ever\n",
      "Topic 14: horror, genre, scary, scare, zombie, monster, original, full\n"
     ]
    }
   ],
   "source": [
    "# Display discovered topics\n",
    "print(\"Discovered Topics:\")\n",
    "print(\"=\" * 80)\n",
    "for idx, topic in lda_model.print_topics(num_topics=NUM_TOPICS, num_words=8):\n",
    "    words = re.findall(r'\"([^\"]+)\"', topic)\n",
    "    print(f\"Topic {idx:2d}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic label mapping - customize based on your discovered topics\n",
    "TOPIC_LABELS = {\n",
    "    0: \"Action/Thriller\", 1: \"Drama/Character\", 2: \"Comedy/Light\", 3: \"Horror/Suspense\",\n",
    "    4: \"Family/Animation\", 5: \"Documentary\", 6: \"Sci-Fi/Fantasy\", 7: \"Romance\",\n",
    "    8: \"Crime/Mystery\", 9: \"War/Historical\", 10: \"Musical/Arts\", 11: \"Sports\",\n",
    "    12: \"International\", 13: \"Adventure/Epic\", 14: \"Classic/Timeless\"\n",
    "}\n",
    "\n",
    "def get_topic_vector(tokens, dictionary, lda_model, num_topics):\n",
    "    bow = dictionary.doc2bow(tokens)\n",
    "    topic_dist = lda_model.get_document_topics(bow, minimum_probability=0)\n",
    "    vec = np.zeros(num_topics)\n",
    "    for topic_id, prob in topic_dist:\n",
    "        vec[topic_id] = prob\n",
    "    return vec\n",
    "\n",
    "def get_generated_genres(topic_vector, top_n=3):\n",
    "    top_indices = np.argsort(topic_vector)[::-1][:top_n]\n",
    "    genres = []\n",
    "    for idx in top_indices:\n",
    "        if topic_vector[idx] > 0.05:\n",
    "            label = TOPIC_LABELS.get(idx, f\"Topic_{idx}\")\n",
    "            pct = int(topic_vector[idx] * 100)\n",
    "            genres.append(f\"{label} ({pct}%)\")\n",
    "    return genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing topic vectors...\n",
      "\n",
      "Example: percy jackson & the olympians: the lightning thief\n",
      "  Generated genres: ['Family/Animation (36%)', 'International (17%)', 'Adventure/Epic (17%)']\n"
     ]
    }
   ],
   "source": [
    "# Compute topic vectors for all movies\n",
    "print(\"Computing topic vectors...\")\n",
    "for movie in vectorized_data:\n",
    "    if 'tokens' not in movie:\n",
    "        combined = movie['movie_title'] + ' ' + movie['movie_info'] + ' ' + movie['combined_review_text']\n",
    "        movie['tokens'] = preprocess_for_lda(clean_text(combined))\n",
    "    movie['topic_vector'] = get_topic_vector(movie['tokens'], dictionary, lda_model, NUM_TOPICS)\n",
    "    movie['generated_genres'] = get_generated_genres(movie['topic_vector'])\n",
    "\n",
    "print(f\"\\nExample: {vectorized_data[0]['movie_title']}\")\n",
    "print(f\"  Generated genres: {vectorized_data[0]['generated_genres']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Similarity Matrix Construction\n",
    "\n",
    "This section builds six separate similarity matrices, each capturing a different dimension of movie similarity:\n",
    "\n",
    "1. **Info Similarity** - Semantic similarity of movie descriptions using cosine similarity on embeddings\n",
    "2. **Content Rating Similarity** - Matches based on age appropriateness (G, PG, PG-13, R, NC-17)\n",
    "3. **Genre Similarity** - Overlap in genre categories (drama, comedy, action, etc.)\n",
    "4. **Year Similarity** - Temporal proximity of release dates\n",
    "5. **Review Style Similarity** - How critics write about movies (embeddings of review text)\n",
    "6. **Review Type Similarity** - Correlation of fresh/rotten patterns across critics\n",
    "\n",
    "Each matrix is NÃ—N where N is the number of movies, with values ranging from 0 (dissimilar) to 1 (identical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_info_sim(vectorized_movie_data) -> np.ndarray:\n",
    "    info_vectors = np.vstack([movie['movie_info_embeddings'] for movie in vectorized_movie_data])\n",
    "    info_sim = cosine_similarity(info_vectors).astype('float32')\n",
    "    print(f\"Info similarity matrix: {info_sim.shape}\")\n",
    "    return info_sim\n",
    "\n",
    "def build_content_rating_sim(vectorized_movie_data) -> np.ndarray:\n",
    "    content_rating_vectors = np.vstack([movie['content_rating_norm'] for movie in vectorized_movie_data])\n",
    "    content_rating_sim = cosine_similarity(content_rating_vectors).astype('float32')\n",
    "    print(f\"Content rating similarity matrix: {content_rating_sim.shape}\")\n",
    "    return content_rating_sim\n",
    "\n",
    "def build_genre_sim(vectorized_movie_data) -> np.ndarray:\n",
    "    genre_vectors = np.vstack([movie['genre_vector'] for movie in vectorized_movie_data])\n",
    "    genre_sim = cosine_similarity(genre_vectors).astype('float32')\n",
    "    print(f\"Genre similarity matrix: {genre_sim.shape}\")\n",
    "    return genre_sim\n",
    "\n",
    "def build_year_sim(vectorized_movie_data) -> np.ndarray:\n",
    "    year_vectors = np.vstack([movie['year_norm'] for movie in vectorized_movie_data])\n",
    "    year_sim = cosine_similarity(year_vectors).astype('float32')\n",
    "    print(f\"Year similarity matrix: {year_sim.shape}\")\n",
    "    return year_sim\n",
    "\n",
    "def build_review_style_sim(vectorized_movie_data) -> np.ndarray:\n",
    "    review_embeddings = np.vstack([movie['avg_review_embeddings'] for movie in vectorized_movie_data])\n",
    "    review_sim = cosine_similarity(review_embeddings).astype('float32')\n",
    "    print(f\"Review similarity matrix: {review_sim.shape}\")\n",
    "    return review_sim\n",
    "\n",
    "def build_review_type_sim(vectorized_movie_data) -> np.ndarray:\n",
    "    all_critics = set()\n",
    "    for movie in vectorized_movie_data:\n",
    "        all_critics.update(movie['critic_names'])\n",
    "    all_critics = sorted(all_critics)\n",
    "\n",
    "    all_movies = [movie['movie_title'] for movie in vectorized_movie_data]\n",
    "    matrix = pd.DataFrame(np.nan, index=all_movies, columns=all_critics)\n",
    "\n",
    "    for movie in vectorized_movie_data:\n",
    "        mit = movie['movie_title']\n",
    "        for critic, review_type in zip(movie['critic_names'], movie['review_types_norm']):\n",
    "            matrix.loc[mit, critic] = review_type\n",
    "\n",
    "    type_sim = matrix.T.corr(method='pearson')\n",
    "    type_sim = (type_sim + 1) / 2\n",
    "    type_sim_array = type_sim.values\n",
    "    type_sim = np.nan_to_num(type_sim_array, nan=0.5)\n",
    "\n",
    "    print(f\"Type similarity matrix: {type_sim.shape}\")\n",
    "    return type_sim\n",
    "\n",
    "def build_sim_matrices(vectorized_data):\n",
    "    info_sim = build_info_sim(vectorized_data)\n",
    "    content_rating_sim = build_content_rating_sim(vectorized_data)\n",
    "    genre_sim = build_genre_sim(vectorized_data)\n",
    "    year_sim = build_year_sim(vectorized_data)\n",
    "    style_sim = build_review_style_sim(vectorized_data)\n",
    "    type_sim = build_review_type_sim(vectorized_data)\n",
    "    return info_sim, content_rating_sim, genre_sim, year_sim, style_sim, type_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading similarity matrices...\n",
      "Matrices ready\n"
     ]
    }
   ],
   "source": [
    "SIMILARITY_PATH = '../cache/similarity_matrices.pkl'\n",
    "\n",
    "if os.path.exists(SIMILARITY_PATH):\n",
    "    print(f\"Loading similarity matrices...\")\n",
    "    with open(SIMILARITY_PATH, 'rb') as f:\n",
    "        matrices = pickle.load(f)\n",
    "    info_sim, content_rating_sim, genre_sim, year_sim, style_sim, type_sim = (\n",
    "        matrices['info'], matrices['content_rating'], matrices['genre'],\n",
    "        matrices['year'], matrices['style'], matrices['type']\n",
    "    )\n",
    "else:\n",
    "    print(\"Computing similarity matrices...\")\n",
    "    info_sim, content_rating_sim, genre_sim, year_sim, style_sim, type_sim = build_sim_matrices(vectorized_data)\n",
    "    with open(SIMILARITY_PATH, 'wb') as f:\n",
    "        pickle.dump({'info': info_sim, 'content_rating': content_rating_sim, 'genre': genre_sim,\n",
    "                     'year': year_sim, 'style': style_sim, 'type': type_sim}, f)\n",
    "print(\"Matrices ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Hybrid Scoring System\n",
    "\n",
    "The hybrid scoring system combines all six similarity matrices using weighted averaging. Each component receives a weight (Î±, Î², Î³, Î´, Îµ, Î¶) that determines its contribution to the final similarity score. The current configuration uses equal weights for the main features:\n",
    "\n",
    "- Î± = 0.2 (Info embeddings - semantic content)\n",
    "- Î² = 0.2 (Content rating)\n",
    "- Î³ = 0.2 (Genre)\n",
    "- Î´ = 0.2 (Release year)\n",
    "- Îµ = 0.1 (Review style)\n",
    "- Î¶ = 0.1 (Review type patterns)\n",
    "\n",
    "This balanced approach gives equal importance to the four primary dimensions (info, rating, genre, year) while treating the review-based features as secondary signals. These weights can be adjusted to emphasize different aspects of similarity depending on the use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid similarity matrix created for 15020 movies\n"
     ]
    }
   ],
   "source": [
    "def hybrid_score(info_sim, content_rating_sim, genre_sim, year_sim, style_sim, type_sim,\n",
    "                 alpha=0.2, beta=0.2, gamma=0.2, delta=0.2, epsilon=0.1, zeta=0.1):\n",
    "    return (alpha*info_sim + beta*content_rating_sim + gamma*genre_sim + \n",
    "            delta*year_sim + epsilon*style_sim + zeta*type_sim)\n",
    "\n",
    "hybrid_sim = hybrid_score(info_sim, content_rating_sim, genre_sim, year_sim, style_sim, type_sim)\n",
    "title_to_idx = {m['movie_title']: i for i, m in enumerate(vectorized_data)}\n",
    "print(f\"Hybrid similarity matrix created for {len(title_to_idx)} movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Clustering Analysis (Course Topic 2)\n",
    "\n",
    "K-Means clustering groups movies based on their genre characteristics. We use standardized genre vectors as input features and evaluate different values of k (number of clusters) using two metrics:\n",
    "\n",
    "- **Silhouette Score** - Measures how well-separated clusters are (higher is better)\n",
    "- **Davies-Bouldin Index** - Measures cluster compactness and separation (lower is better)\n",
    "\n",
    "The optimal k is selected based on these metrics, and each movie is assigned to a cluster. Movies in the same cluster share similar genre profiles, providing an alternative grouping strategy to complement the similarity-based recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_matrix = np.vstack([m['genre_vector'] for m in vectorized_data])\n",
    "scaler = StandardScaler()\n",
    "genre_scaled = scaler.fit_transform(genre_matrix)\n",
    "\n",
    "# Find optimal k\n",
    "k_range = range(2, 16)\n",
    "silhouettes = [silhouette_score(genre_scaled, KMeans(n_clusters=k, random_state=42, n_init=10).fit_predict(genre_scaled)) for k in k_range]\n",
    "db_scores = [davies_bouldin_score(genre_scaled, KMeans(n_clusters=k, random_state=42, n_init=10).fit_predict(genre_scaled)) for k in k_range]\n",
    "\n",
    "print(f\"Optimal k by Silhouette: {list(k_range)[np.argmax(silhouettes)]}\")\n",
    "print(f\"Optimal k by Davies-Bouldin: {list(k_range)[np.argmin(db_scores)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final clustering\n",
    "OPTIMAL_K = 8\n",
    "kmeans = KMeans(n_clusters=OPTIMAL_K, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(genre_scaled)\n",
    "\n",
    "for i, m in enumerate(vectorized_data):\n",
    "    m['cluster'] = cluster_labels[i]\n",
    "\n",
    "print(f\"\\nCluster distribution:\")\n",
    "for c, cnt in zip(*np.unique(cluster_labels, return_counts=True)):\n",
    "    print(f\"  Cluster {c}: {cnt} movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Final Recommendation System\n",
    "\n",
    "The final recommendation system integrates all three components:\n",
    "\n",
    "1. **Similar Items** - Returns the top k most similar movies based on hybrid similarity scores\n",
    "2. **Generated Genres** - Displays LDA-discovered topic labels for each recommendation\n",
    "3. **Cluster Members** - Shows other movies from the same K-Means cluster\n",
    "\n",
    "For each query movie, the system displays:\n",
    "- Original metadata (year, content rating, genres)\n",
    "- LDA-generated genre labels based on review content\n",
    "- Top k similar movies with similarity scores and their generated genres\n",
    "- Other movies sharing the same cluster assignment\n",
    "\n",
    "This multi-faceted approach provides comprehensive recommendations from different analytical perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_movies(query_title, vectorized_data, hybrid_sim, title_to_idx, k=10):\n",
    "    if query_title not in title_to_idx:\n",
    "        suggestions = [t for t in title_to_idx.keys() if query_title in t][:5]\n",
    "        print(f\"Movie '{query_title}' not found. Suggestions: {suggestions}\")\n",
    "        return []\n",
    "    \n",
    "    q_idx = title_to_idx[query_title]\n",
    "    scores = hybrid_sim[q_idx]\n",
    "    sorted_idx = np.argsort(scores)[::-1]\n",
    "    \n",
    "    recs = [(vectorized_data[i], scores[i]) for i in sorted_idx if i != q_idx][:k]\n",
    "    return recs\n",
    "\n",
    "def get_cluster_members(query_title, vectorized_data, title_to_idx, max_n=5):\n",
    "    if query_title not in title_to_idx:\n",
    "        return []\n",
    "    q_cluster = vectorized_data[title_to_idx[query_title]]['cluster']\n",
    "    return [m for m in vectorized_data if m['cluster'] == q_cluster and m['movie_title'] != query_title][:max_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_recommendations(query_title, vectorized_data, hybrid_sim, title_to_idx, k=10):\n",
    "    \"\"\"\n",
    "    Display comprehensive recommendations:\n",
    "    - Similar movies with scores and generated genres\n",
    "    - Movies from the same cluster\n",
    "    \"\"\"\n",
    "    if query_title not in title_to_idx:\n",
    "        print(f\"Movie '{query_title}' not found\")\n",
    "        return\n",
    "    \n",
    "    query = vectorized_data[title_to_idx[query_title]]\n",
    "    \n",
    "    # Header\n",
    "    print(\"\\n\" + \"=\" * 95)\n",
    "    print(f\"  RECOMMENDATIONS FOR: {query_title.upper()}\")\n",
    "    print(\"=\" * 95)\n",
    "    print(f\"  Year: {query['year']}  |  Rating: {query['content_rating']}  |  Cluster: {query['cluster']}\")\n",
    "    print(f\"  Original Genres: {query['genres']}\")\n",
    "    print(f\"  Generated Genres (LDA): {', '.join(query.get('generated_genres', ['N/A']))}\")\n",
    "    print(\"=\" * 95)\n",
    "    \n",
    "    # Similar Movies\n",
    "    similar = get_similar_movies(query_title, vectorized_data, hybrid_sim, title_to_idx, k)\n",
    "    \n",
    "    print(f\"\\n  TOP {k} SIMILAR MOVIES\")\n",
    "    print(\"  \" + \"-\" * 91)\n",
    "    print(f\"  {'RANK':<5}{'TITLE':<40}{'SCORE':<8}{'YEAR':<6}{'GENERATED GENRES (LDA)'}\")\n",
    "    print(\"  \" + \"-\" * 91)\n",
    "    \n",
    "    for i, (m, score) in enumerate(similar, 1):\n",
    "        title = m['movie_title'][:37] + \"...\" if len(m['movie_title']) > 40 else m['movie_title']\n",
    "        genres = ', '.join(m.get('generated_genres', ['N/A'])[:2])\n",
    "        print(f\"  {i:<5}{title:<40}{score:<8.4f}{m['year']:<6}{genres}\")\n",
    "    \n",
    "    # Cluster Members\n",
    "    cluster_members = get_cluster_members(query_title, vectorized_data, title_to_idx, 5)\n",
    "    \n",
    "    print(f\"\\n  OTHER MOVIES IN CLUSTER {query['cluster']}\")\n",
    "    print(\"  \" + \"-\" * 91)\n",
    "    \n",
    "    if cluster_members:\n",
    "        for m in cluster_members:\n",
    "            title = m['movie_title'][:45] + \"...\" if len(m['movie_title']) > 48 else m['movie_title']\n",
    "            print(f\"  â€¢ {title:<50} ({m['year']}) - {m['genres'][:35]}\")\n",
    "    else:\n",
    "        print(\"  No other movies in this cluster.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the recommendation system\n",
    "display_recommendations('aliens', vectorized_data, hybrid_sim, title_to_idx, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_recommendations('toy story', vectorized_data, hybrid_sim, title_to_idx, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_recommendations('the godfather', vectorized_data, hybrid_sim, title_to_idx, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Results and Evaluation\n",
    "\n",
    "This section provides a summary of the complete system, including dataset statistics, the similarity components used, clustering configuration, and topic modeling setup. The evaluation is primarily qualitative, examining whether recommendations align with expected movie relationships. Quantitative metrics like silhouette scores and Davies-Bouldin index are used to validate the clustering component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PROJECT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset: {len(vectorized_data):,} movies, {sum(len(m['reviews']) for m in vectorized_data):,} reviews\")\n",
    "print(f\"\\nSimilarity Components: Info embeddings, Content rating, Genre, Year, Review style, Review type\")\n",
    "print(f\"\\nClustering: K-Means with k={OPTIMAL_K}\")\n",
    "print(f\"\\nOutside Topic: LDA with {NUM_TOPICS} topics for automatic genre generation\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
